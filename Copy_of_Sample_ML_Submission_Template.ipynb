{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " **Name**  Anjali Prasad\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " **contribution** individual\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Bike demand prediction is a common problem faced by bike rental companies, as accurately forecasting the demand for bikes can help optimize inventory and pricing strategies. In this project, I aim to develop a regression supervised machine learning model to predict the demand for bikes in a given time period.\n",
        "\n",
        "Originally dataset of bike rental information from a bike sharing company, had information including details on the number of bikes rented, the time and date of the rental, and various weather and seasonality features, information on other relevant factors that could impact bike demand, such as holidays, functioning or non functioning day.\n",
        "\n",
        "After preprocessing and cleaning the data, I split it into training and test sets and used the training data to train our machine leaming model. I experimented with several different model architectures and hyperparameter settings, ultimately selecting the model that performed the best on the test data.\n",
        "\n",
        "To evaluate the performance of our model, I used a variety of metrics, including mean absolute error, root mean squared error, and R-squared. I\n",
        "\n",
        "found that our model was able to make highly accurate predictions, with an R-squared value of 0.88 and a mean absolute error of just 2.58.\n",
        "\n",
        "In addition to evaluating the performance of our model on the test data, I also conducted a series of ablation studies to understand the impact of individual features on the model's performance. I found that the temperature, as well as the weather and seasonality features, had the greatest impact on bike demand.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jdA2CxvVq4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concem. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "\n",
        "My goal is to develop a model that is highly accurate, with a low mean absolute error and a high R-squared value. The model should also be able to provide insights into the factors that most impact bike demand, helping the bike sharing company to make data-driven decisions about how to optimize their operations"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "#Datetime library for manipulating Date columns.\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "# from sci-kit library scaling, transforming and labeling functions.\n",
        "# which is used to change raw feature vectors into a representation\n",
        "\n",
        "#suitable for the downstream estimators.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "#Importing various machine learning models. from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#import different metrics from sci-kit libraries for model evaluation.\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "#Importing warnings library. The warnings module handles warnings in Python.\n",
        "import warnings\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df=pd.read_csv('/content/drive/MyDrive/SeoulBikeData.csv',encoding='latin')"
      ],
      "metadata": {
        "id": "HR3fFkJ3BoDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.tail()"
      ],
      "metadata": {
        "id": "Z67xST7QCF5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(bike_df.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting all columns\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "iDXbLZAFCZgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Data is duplicated? (bike_df.duplicated().value_counts()), unique values with (len(bike_df[bike_df.duplicated()])) duplication\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bike_df.columns.tolist():\n",
        "\n",
        "  print(f\"No. of unique values in {i} is {bike_df[i].nunique()}.\")"
      ],
      "metadata": {
        "id": "5qR5zR_EI01o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(bike_df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer In a day we have 24 hours and we have 365 days a year so 365 multiplied by 24-8760, which represents the number of line in the dataset\n",
        "\n",
        "There are no null values.\n",
        "\n",
        "Dataset has all unique values Le, there is no duplicate, which means data is free from bias as duplicates which can cause problems in downstream analysis, such as biasing results or making it difficult to accurately summarize the data.\n",
        "\n",
        "Date has some object data types, it should be datetime data type."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f'Features: {bike_df.columns.to_list()}')"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Breakdown of Our Features:\n",
        "\n",
        "Date: The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type: str, we need to convert into datetime format.\n",
        "\n",
        "Rented Bike Count: Number of rented bikes per hour which our dependent variable and we need to predict that, type: int\n",
        "\n",
        "Hour: The hour of the day, starting from 0-23 it's in a digital time format, type: int, we need to convert it into category data type.\n",
        "\n",
        "Temperature(\"C): Temperature in Celsius, type: Float\n",
        "\n",
        "Humidity(%): Humidity in the air in %, type: int\n",
        "\n",
        "Wind speed (m/s): Speed of the wind in m/s, type: Float\n",
        "\n",
        "Visibility (10m): Visibility in m, type: int\n",
        "\n",
        "Dew point temperature (°C): Temperature at the beggining of the day, type: Float\n",
        "\n",
        "Solar Radiation (MJ/m2): Sun contribution, type: Float\n",
        "\n",
        "Rainfall(mm): Amount of raining in mm, type: Float\n",
        "\n",
        "Snowfall (cm): Amount of snowing in om, type: Float\n",
        "\n",
        "Seasons: \"Season of the year, type: stc, there are only 4 seseori's in dets\",\n",
        "\n",
        "Holiday: If the day is holiday period or not, type: str\n",
        "\n",
        "Functioning Day. If the day is a Functioning Day or not, type: str"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing/null values"
      ],
      "metadata": {
        "id": "a-DhTVonLlRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "1eYyaba3LvRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing missing values\n",
        "missing =pd.DataFrame((bike_df.isnull().sum())*100/bike_df.shape[0]).reset_index()\n",
        "\n",
        "plt.figure(figsize=(16,5))\n",
        "\n",
        "ax= sns.pointplot(x='index', y=0, data=missing)\n",
        "\n",
        "plt.xticks(rotation=90, fontsize=7)\n",
        "\n",
        "plt.title(\"Percentage of Missing values\")\n",
        "\n",
        "plt.ylabel(\"PERCENTAGE\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v0TbCSfeOUh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# duplicate values\n",
        "value=len(bike_df [bike_df.duplicated()])\n",
        "\n",
        "print(\"The number of duplicate values in the data set is = \",value)\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename column complex names\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count': 'Rented_Bike_Count',\n",
        "\n",
        "'Temperature(°C)': 'Temperature',\n",
        "\n",
        "\"Humidity (%)\": \"Humidity\",\n",
        "\n",
        "'Wind speed(m/s)': 'wind_speed',\n",
        "\n",
        "'Visibility(10m)': 'Visibility',\n",
        "\n",
        "'Dew point temperature (°C)': 'Dew_point_temperature',\n",
        "\n",
        "'Solar Radiation (M3/m2)': 'Solar_Radiation',\n",
        "'Rainfall(mm)': 'Rainfall',\n",
        "\n",
        "'Snowfall (cm)': 'Snowfall',\n",
        "\n",
        "'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "syiHjfAEPCWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# breakdown column\n",
        "bike_df['Date']= bike_df['Date'].str.replace('-', '/')\n",
        "\n",
        "bike_df['Date']= bike_df['Date'].apply(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))"
      ],
      "metadata": {
        "id": "-1PTjMlOPCTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['year']= bike_df[\"Date\"].dt.year\n",
        "bike_df ['month'] =bike_df['Date'].dt.month\n",
        "\n",
        "bike_df[ 'day']= bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "7q368AhzPCPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df[\"weekdays weekend\"]=bike_df['day'].apply(lambda x: 1 if x== 'Saturday' or x=='Sunday' else 0)\n",
        "\n",
        "bike_df=bike_df.drop(columns=['Date', 'day', 'year'], axis=1)\n"
      ],
      "metadata": {
        "id": "NmfHikXkPCNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head()"
      ],
      "metadata": {
        "id": "qbbc5gybPCJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df[\"weekdays weekend\"].value_counts()"
      ],
      "metadata": {
        "id": "U56wt2Q-PCHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing datatype\n",
        "cols=['Hour', 'month', 'weekdays weekend']\n",
        "for col in cols:\n",
        "  bike_df[col]=bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "sWcoh6oTPCFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "wtpxUbKQPCBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.columns"
      ],
      "metadata": {
        "id": "1Pipr3d8PBwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qGn6HLm9Wb1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig,ax= plt.subplots(figsize=(12,7))\n",
        "sns.barplot(data=bike_df , x='month', y='Rented_Bike_Count', ax=ax, capsize=.2)\n",
        "ax.set(title='count of rented bikes according to month')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer i choose bar graph to show count of rented bike according to month\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here we can clearly see the demand is higher during may to october(5-10)"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig,ax= plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df , x='weekdays weekend', y='Rented_Bike_Count', ax=ax, capsize=.2)\n",
        "ax.set(title='count of rented bikes according to weekdays weekend')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig,ax= plt.subplots(figsize=(12,7))\n",
        "sns.pointplot(data=bike_df , x='Hour', y='Rented_Bike_Count', hue= 'weekdays weekend', ax=ax)\n",
        "ax.set(title='count of rented bikes according to weekdays_weekends')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer i used point plot to show demands of bike"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer we can see that week days which reprents bluecolor shows demand in bike."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fig,ax= plt.subplots(figsize=(12,7))\n",
        "sns.barplot(data=bike_df , x='Hour', y='Rented_Bike_Count', ax=ax , capsize=.2)\n",
        "ax.set(title='count of rented bikes according to Hour')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer i choose this graph to show count of rented bikes according to Hour."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here we can see that generally peaple are using thier bike in working hours from 7 am to 9 pm."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "fig,ax= plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df , x='Functioning_Day', y='Rented_Bike_Count', ax=ax , capsize=.2)\n",
        "ax.set(title='count of rented bikes according to Functioning day')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer i choose bar graph to show count of rented bikes according to Functioning day"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig,ax= plt.subplots(figsize=(12,7))\n",
        "sns.pointplot(data=bike_df , x='Hour', y='Rented_Bike_Count', hue='Functioning_Day', ax=ax)\n",
        "ax.set(title='count of rented bikes according to Functioning_Day')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer i choose point graph to count of rented bikes according to Functioning_Day"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here we can see that peaple dont use bike in no functioning day"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "fig,ax= plt.subplots(figsize=(12,6))\n",
        "sns.barplot(data=bike_df , x='Seasons', y='Rented_Bike_Count', ax=ax , capsize=.2)\n",
        "ax.set(title='count of rented bikes according to Functioning seasons')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "fig,ax= plt.subplots(figsize=(12,6))\n",
        "sns.pointplot(data=bike_df , x='Hour', y='Rented_Bike_Count', hue='Seasons', ax=ax)\n",
        "ax.set(title='count of rented bikes according to Seasons')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Winter has least number of bike counts"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "fig,ax= plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df , x='Holiday', y='Rented_Bike_Count', ax=ax , capsize=.2)\n",
        "ax.set(title='count of rented bikes according to Holiday')"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "fig,ax= plt.subplots(figsize=(12,6))\n",
        "sns.pointplot(data=bike_df , x='Hour', y='Rented_Bike_Count', hue='Holiday', ax=ax)\n",
        "ax.set(title='count of rented bikes according to Holiday')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer in holiday peaple uses rented bike from 2pm-8pm"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  seperate numeric value from dataframe\n",
        "numeric_features=bike_df.select_dtypes(exclude=['object' , 'category'])\n",
        "\n",
        "numeric_features"
      ],
      "metadata": {
        "id": "60wzkmZYgnKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "plt.figure(figsize=(15,10))\n",
        "for i in numeric_features.columns:\n",
        "  plt.subplot(3,3,n)\n",
        "  n=n+1\n",
        "  sns.distplot(bike_df[i])\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "H_-ZdXEXjDGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.groupby('Temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "4fvloQ4uuRFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.groupby('Dew point temperature(°C)').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "fqhjvkKywDA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.groupby('Solar Radiation (MJ/m2)').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "C-GU6D7v0_PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.groupby('Snowfall').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "Cp7hLtbt0_L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.groupby('Wind speed (m/s)').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "k-gO22gR1bz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.groupby('Rainfall').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "xKdVujEP7bab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression** **plot**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regression plots in seabom are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. Regression plots as the name suggests creates a regression line between 2 parameters and helps to visualize their linear relationships."
      ],
      "metadata": {
        "id": "0cDMPV7T6MQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the regression plot for all the numerical features\n",
        "\n",
        "for col in numeric_features:\n",
        "\n",
        "  fig,ax=plt.subplots(figsize=(8,4))\n",
        "\n",
        "  sns.regplot(x=bike_df[col], y=bike_df['Rented_Bike_Count'], scatter_kws={\"color\": 'orange'}, line_kws={\"color\": \"black\"})"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above regression plot of all numerical features we see that the columne 'Temperature, 'Wind speed, Visibility, Dew point temperature, 'Solar Radiation' are positively relation to the target variable.\n",
        "\n",
        "which means the rented bike count increases with increase of these features.\n",
        "\n",
        "Rainfall, Snowfall, Humidity' these features are negatively related with the target varlaable which means the rented bike count decreases when these features increase\n"
      ],
      "metadata": {
        "id": "ReI-bR8j8P9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Normalise Rented_Bike_Count column data**\n",
        "\n",
        "The data normalization (also referred to ae data pre-processing) is a basic element of data mining. It means transforming the data, namely converting the source data in to another format that allows processing data offlectively. The main purpose of data normalization is to minimize or even exclude duplicated data"
      ],
      "metadata": {
        "id": "1net9iqR8VPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Distribution plot of Rented Bike Count\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.xlabel('Rented_Bike_Count')\n",
        "\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.distplot(bike_df['Rented_Bike_Count'], hist=True,color=\"y\")\n",
        "\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].mean(), color=\"magenta\", linestyle=\"dashed\", linewidth=2)\n",
        "\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].median(), color='black', linestyle=\"dashed\", linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fTVjy36Z8geL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above graph shows that, Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal, so we should perform some operation to make it normal."
      ],
      "metadata": {
        "id": "iMIqeBup9BIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Finding Outliers and treatment **\n",
        "\n"
      ],
      "metadata": {
        "id": "jNTF-nSa9DIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot for Rented bike Count to check outliers\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "\n",
        "sns.boxplot(x=bike_df['Rented_Bike_Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q83USPRW9P46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outliers treatments\n",
        "\n",
        "bike_df.loc[bike_df['Rainfall']>=4, 'Rainfall']= 4\n",
        "\n",
        "bike_df.loc[bike_df['Solar Radiation (MJ/m2)']>=2.5, 'Solar_Radiation']=2.5\n",
        "\n",
        "bike_df.loc[bike_df['Snowfall']>2, 'Snowfall']= 2\n",
        "\n",
        "bike_df.loc[bike_df[ 'Wind speed (m/s)']>=4, 'Wind_speed']= 4"
      ],
      "metadata": {
        "id": "DI6ovLQf-mM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have applied outlier treatment techniques to the dataset by replacing the outliers with the maximum values."
      ],
      "metadata": {
        "id": "teZuGS2t_TcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying square root to Rented Bike Count to improve skewness\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "plt.xlabel('Rented Bike Count')\n",
        "\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.distplot(np.sqrt(bike_df['Rented_Bike_Count']), color=\"y\")\n",
        "\n",
        "\n",
        "\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).mean(), color=\"magenta\", linestyle='dashed', linewidth=2)\n",
        "\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSp1rAop-lYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have generic rule of applying Square root for the skewed variable in order to make it normal.After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "4gW4PqDU_9-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#after applying sqrt on Rented Bike Count check wheater we still have outliers\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "\n",
        "sns.boxplot(x=np.sqrt(bike_df[ 'Rented_Bike_Count']))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UAHbNWrYAB0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.corr()"
      ],
      "metadata": {
        "id": "9SDZTonJALTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking of Correlation between variables\n",
        "\n",
        "Checking in OLS Model\n",
        "\n",
        "Ordinary least squares (OLS) regression is a statistic variables and a dependent variable the relationship between one or more indepen\n",
        "\n"
      ],
      "metadata": {
        "id": "o7aYHBhAAhx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the module\n",
        "\n",
        "#assign the 'x','y' value\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "X=bike_df[[ 'Temperature', 'Humidity(%)', \"Wind speed (m/s)\",'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall', 'Snowfall']]\n",
        "Y = bike_df['Rented_Bike_Count']\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "kWoWHE31ALD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=sm.add_constant(X)\n",
        "X"
      ],
      "metadata": {
        "id": "UH1CSPSrBZMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=sm.OLS(Y,X).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IrKFgtf4BY9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.corr()"
      ],
      "metadata": {
        "id": "n4t3TOKjBYuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " From the OLS model we find that the \"Temperature' and 'Dew point temperature are highly correlated so we need to drop one of them. For droping them we check the (P>(tl) value from above table and we can see that the 'Dew point temperature' value is higher so we need to drop Dew point temperature column\n"
      ],
      "metadata": {
        "id": "wfh8ueLTFlF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap\n",
        "\n",
        "A correlation Heatmap is a type of graphical representation that displays the correlation matrix, which helps to determine the correlation between different variables."
      ],
      "metadata": {
        "id": "Z2SpHbkAF4gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(bike_df.corr(),cmap='PiYG' ,annot=True)"
      ],
      "metadata": {
        "id": "F4UxnO1PFopd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AE7Pfa4iHsSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df=bike_df.drop(['Dew point temperature(°C)'], axis=1)"
      ],
      "metadata": {
        "id": "wuM1feGSHDVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "rOGMpaQmHDSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe on the heatmap that on the target variable line, the most positively correlated variables to the rent are:\n",
        "\n",
        "⚫ the temperature\n",
        "\n",
        "• the dew point temperature\n",
        "\n",
        "• the solar radiation\n",
        "\n",
        "And most negatively correlated variables are:\n",
        "\n",
        "⚫ humidity\n",
        "\n",
        "• rainfall\n",
        "\n",
        "• From the above correlation heatmap, We see that there is a positive correlation between columns Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it won't affect the outcome of our analysis. And they have the same variations we can drop the column 'Dew point temperature(\"C)."
      ],
      "metadata": {
        "id": "Kn4VV_kCIPvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering & Data Pre-processing\n",
        "\n",
        "Create the dummy variables\n",
        "\n",
        "A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use these ob programming efficiently we crosto durmeny variables.\n",
        "\n",
        "One Hot Encoding"
      ],
      "metadata": {
        "id": "_VUO3AALPnxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign all categorical features to a variable\n",
        "\n",
        "categorical_features=list(bike_df.select_dtypes(['object', 'category']).columns)\n",
        "categorical_features=pd. Index(categorical_features)\n",
        "\n",
        "categorical_features\n",
        "\n"
      ],
      "metadata": {
        "id": "kexTCUGNPsCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "one hot encoding\n",
        "\n",
        "A ane hot encoding allows the representation of estagorical data to bo morod. any nuachène learning algorithma cannot work with categorical data directly. The categoriam mcast be converted into mambers. This is required for both leput and output variables that are categorical."
      ],
      "metadata": {
        "id": "vNBhSph_QYeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a copy\n",
        "\n",
        "\n",
        "bike_df_copy= bike_df\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "\n",
        "  data= pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "  data= data.drop([column], axis=1)\n",
        "  return data\n",
        "\n",
        "for col in categorical_features:\n",
        "  bike_df_copy =one_hot_encoding(bike_df_copy, col)\n",
        "\n",
        "bike_df_copy.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "jfp8q2hbQYCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model Training\n",
        "\n",
        "Train Test split for regression\n",
        "\n",
        "Before, fitting any model it is a rule of thumb to spligthe dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library.\n"
      ],
      "metadata": {
        "id": "P68sy1gwROal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the value in X and Y\n",
        "\n",
        "x=bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y=np.sqrt(bike_df_copy['Rented_Bike_Count'])\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "hIENROqkRSTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "r1sMYodXRr_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create test and train data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "cPPnParcRrvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.info()"
      ],
      "metadata": {
        "id": "pZWpu_o3Slxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.describe().columns"
      ],
      "metadata": {
        "id": "BXSyRYrYSlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the \"errors\") and squaring them. It's called the mean squared error as you're finding the average of a set of errors. The lower the MSE, the better the forecast.\n",
        "\n",
        "MSE formula (1/n) 2(actual-forecast)2 Where:\n",
        "\n",
        "nnumber of itens,\n",
        "\n",
        "summation notation,\n",
        "\n",
        "Actual original or observed y-value,\n",
        "\n",
        "Forecasty-value from regression.\n",
        "\n",
        "• Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors).\n",
        "\n",
        "• Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model.... Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable.\n",
        "\n",
        "R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
        "\n",
        "• Formula for R-Squared\n",
        "\n",
        "B-1- Unexplained Variation Total Variation\n",
        "\n",
        "R2-1-Total Variation Unexplained Variation\n",
        "\n",
        "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        " LINEAR REGRESSION\n",
        "\n",
        "Regression models describe the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line\n",
        "\n",
        "Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independent variables and dependent variable. In case of single variable, the formula is same as straight line equation having an intercept and slope.\n",
        "\n",
        "y_pred - βo + βια\n",
        "\n",
        "where\n",
        "\n",
        "Bo and Bi\n",
        "\n",
        "are intercept and slope respectively.\n",
        "\n",
        "In case of multiple features the formula translates into:\n",
        "\n",
        "y_pred Bo+Biz +++....\n",
        "\n",
        "where x1,x2,x3 are the features values and\n",
        "\n",
        "βος βι. β..... are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent."
      ],
      "metadata": {
        "id": "V7zU_Nm2S0dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg =LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NB_2O5QLTSml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #check the score\n",
        "reg.score(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "h6L7Tcq_TSZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #check the coefficeint\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "Cx08eV-nTR9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the x train and X-test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)\n"
      ],
      "metadata": {
        "id": "bg_dXkmCXueu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #import the packages\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE:\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"AISE:\", RMSE_lr)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"HAE:\", MAE_lr)\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2:\",r2_lr)\n",
        "\n",
        "Adjusted_R2_lr= (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2:\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "FxlKK8-kXuO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict1={'Model': 'Linear regression',\n",
        "\n",
        "'MAE': round((MAE_lr),3),\n",
        "\n",
        "'MSE': round((MSE_lr), 3),\n",
        "\n",
        "'RMSE':round((RMSE_lr),3),\n",
        "\n",
        "'R2_score':round((r2_lr),3),\n",
        "\n",
        "'Adjusted_R2':round((Adjusted_R2_lr),2)\n",
        "\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1, index=[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "IUcJBHb6lEnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print(\"MSE:\", MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "\n",
        "print(\"RMSE:\", RMSE_lr)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "print(\"MAE:\",MAE_lr)\n",
        "\n",
        "#import the packages\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#calculate r2 and adjusted r2\n",
        "\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "\n",
        "Adjusted_R2_lr= (1-(1-r2_score(y_test, y_pred_test))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\", Adjusted_R2_lr)"
      ],
      "metadata": {
        "id": "yJA46bPolEAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict2={'Model': 'Linear regression',\n",
        "\n",
        "'MAE': round((MAE_lr),3),\n",
        "\n",
        "'MSE': round((MSE_lr), 3),\n",
        "\n",
        "'RMSE':round((RMSE_lr),3),\n",
        "\n",
        "'R2_score':round((r2_lr),3),\n",
        "\n",
        "'Adjusted_R2':round((Adjusted_R2_lr),2)\n",
        "\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict2, index=[1])"
      ],
      "metadata": {
        "id": "3UD0m57J_Pm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ### Heteroscadacity Residual plot\n",
        "\n",
        "plt.scatter((y_pred_test), (y_test)-(y_pred_test))\n",
        "\n",
        "plt.xlabel('Predicted Values')\n",
        "\n",
        "plt.ylabel('Residuals')\n",
        "\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tx668ZFE_PR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual Price vs predicte for Linear Regression plot\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.plot(y_pred_test)\n",
        "\n",
        "plt.plot(np.array(y_test))\n",
        "\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "plt.xlabel('No of Test Data')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bYM7bAZJIzxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression"
      ],
      "metadata": {
        "id": "0DJCbhIFOCk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "metadata": {
        "id": "AQbvLGwbIzsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[78] # Create an instance of Lasso Regression implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso= Lasso(alpha=1.0, max_iter=3000)\n",
        "\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X_train, y_train)\n",
        "#Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ],
      "metadata": {
        "id": "T8ypwTXwIzod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "C0hsNe-OIzle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE:\", MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"AMSE:\", RMSE_l)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_l=mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE:\",MAE_l)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#calculate r2 and adjusted r2\n",
        "\n",
        "r2_l=r2_score(y_train, y_pred_train_lasso)\n",
        "\n",
        "print(\"R2 :\",r2_l)\n",
        "\n",
        "Adjusted_R2_l= (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "8GSNgH-NIzGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict1={'Model': 'Lasso regression', 'MAE':round((MAE_l),3),\n",
        "\n",
        "'MSE':round((MSE_l),3),\n",
        "\n",
        "'RMSE': round((RMSE_l),3),\n",
        "\n",
        "'R2_score':round((r2_l),3),\n",
        "\n",
        "'Adjusted R2': round((Adjusted_R2_l),2)\n",
        "\n",
        "}\n",
        "training_df=training_df.append(dict1, ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "e2I4OKy7XPuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_l=mean_squared_error(y_test, y_pred_test_lasso)\n",
        "\n",
        "print(\"MSE:\",MSE_l)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE:\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE\", RMSE_l)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_1= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE:\",MAE_l)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#calculate r2 and adjusted r2\n",
        "\n",
        "r2_l=r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2:\",r2_l)\n",
        "Adjusted_R2_l= (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "lmF8aTXVXPq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict2={'Model': 'Lasso regression',\n",
        "\n",
        "      'MAE': round((MAE_l),3),\n",
        "      'MSE': round((MSE_l),3),\n",
        "\n",
        "      'RMSE': round((RMSE_l),3),\n",
        "\n",
        "      'R2_score': round((r2_l),3),\n",
        "\n",
        "      'Adjusted R2':round((Adjusted_R2_l),2),\n",
        "\n",
        "    }\n",
        "test_df=test_df.append(dict2, ignored_text=True)\n"
      ],
      "metadata": {
        "id": "Z1rPDeWlXPoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ### Heteroscadacity- Residual plot\n",
        "plt.scatter((y_pred_test_lasso), (y_test-y_pred_test_lasso))\n",
        "\n",
        "plt.xlabel('Predicted Values')\n",
        "\n",
        "plt.ylabel('Residuals')\n",
        "\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G90O44DVXPlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "\n",
        "plt.plot(np.array((y_test)))\n",
        "\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ecVYt6WcXPi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RIDGE REGRESSION\n",
        "\n",
        "Ridge regression is a method of estimating the coefficients of regression models in scenarios where the independent variables are highly correlated. It uses the linear regression model with the L2 regularization method."
      ],
      "metadata": {
        "id": "TL3x3E46f1RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge= Ridge(alpha=0.1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2GjoMOORXPgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "ridge.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "CqgDgfutXPcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "ridge.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "c3AkoDIcgzYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "M4Gqo3HkgzVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_ridge"
      ],
      "metadata": {
        "id": "gjNy_5ZGgzR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test_ridge"
      ],
      "metadata": {
        "id": "OrSIlMothE8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_r=mean_squared_error(y_test, y_pred_test_ridge)\n",
        "\n",
        "print(\"MSE:\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE\", RMSE_r)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE:\",MAE_r)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#calculate r2 and adjusted r2\n",
        "\n",
        "r2_r=r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2:\",r2_l)\n",
        "Adjusted_R2_r= (1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "PayPAWmKhuTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict1={'Model': 'Ridge regression',\n",
        "\n",
        "'MAE': round((MAE_r),3),\n",
        "\n",
        "'MSE':round((MSE_r),3),\n",
        "\n",
        "'RMSE':round((RMSE_r),3),\n",
        "\n",
        "'R2_score': round((r2_r),3),\n",
        "\n",
        "'Adjusted R2':round((Adjusted_R2_r),2)}\n",
        "\n",
        "training_df=training_df.append(dict1, ignore_index=True)"
      ],
      "metadata": {
        "id": "uNjAhu13i-ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #import the packages from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_r =mean_squared_error(y_test, y_pred_test_ridge)\n",
        "\n",
        "print(\"MSE:\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE:\", RMSE_r)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_r =mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE:\", MAE_r)\n",
        "\n",
        "#import the packages from sklearn.metrics import r2_score #calculate r2 and adjusted: r2 r2_r=r2_score((y_test), (y_pred_test_ridge))"
      ],
      "metadata": {
        "id": "QUDPE2o7i94-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R2 :\",r2_r)\n",
        "\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "rEhfJdSYi9qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict2={'Model': 'Ridge regression',\n",
        "\n",
        "        'MAE':round ((MAE_r),3),\n",
        "\n",
        "        'MSE': round((MSE_r),3),\n",
        "\n",
        "        'RMSE' : round((RMSE_r),3),\n",
        "\n",
        "        'R2_score': round((r2_r),3),\n",
        "\n",
        "        'Adjusted R2': round((Adjusted_R2_r),2)}\n",
        "\n",
        "test_df=test_df.append(dict2, ignore_index=True)"
      ],
      "metadata": {
        "id": "-sP1v1Ji1wuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "\n",
        "plt.scatter((y_pred_test_ridge), (y_test)-(y_pred_test_ridge))\n",
        "\n",
        "plt.xlabel('Predicted Values')\n",
        "\n",
        "plt.ylabel('Residuals')\n",
        "\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_EPBCHCz1wms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.plot((y_pred_test_ridge))\n",
        "\n",
        "plt.plot((np.array(y_test)))\n",
        "\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "--WAl5MU1wip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELASTIC NET REGRESSION\n",
        "\n",
        "\n",
        "Elastic Net regression is a linear regression model that combines both L1 (Lasso) and L2 (Ridge) regularization penalties to overcome some of the limitations of each individual method.\n",
        "\n",
        "The model introduces two hyperparameters, alpha and 11 ratio, which control the strength of the L1 and 12 penalties, respectively. Elastic Nat mgression is particularly useful when dealing with datasets that have high dimensionality and multicollinearity between features."
      ],
      "metadata": {
        "id": "KwqM2Qns4D-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "#a L1+ b 12\n",
        "\n",
        "#alpha a + b and 11_ratio= a/(a+b)\n",
        "elasticnet =ElasticNet(alpha=0.1, l1_ratio=0.5)\n"
      ],
      "metadata": {
        "id": "buG_KoMw1weU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elasticnet.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "XHbe7iZq1wac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "ebIpTEk71wWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_en=elasticnet.predict(X_train)\n",
        "y_pred_test_en=elasticnet.predict(X_test)\n",
        "\n",
        "print(y_pred_train_en)\n",
        "print(y_pred_test_en)"
      ],
      "metadata": {
        "id": "HRmHPEYN1wSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#import the packages\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE:\", MSE_e)\n",
        "\n",
        "#calculate RASE\n",
        "\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE:\", RMSE_e)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_e=mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE:\", MAE_e)\n",
        "\n",
        "#import the packages\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#calculate r2 and adjusted r2\n",
        "\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "\n",
        "print(\"R2:\",r2_e)\n",
        "\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n"
      ],
      "metadata": {
        "id": "FtxITbSW1wNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the test set metrics value in a dataframe for later compari\n",
        "\n",
        "dict1={'Model': 'Elastic net regression',\n",
        "\n",
        "      'MAE': round((MAE_e),3),\n",
        "\n",
        "      'MSE': round((MSE_e),3),\n",
        "\n",
        "      'RMSE': round((RMSE_e),3),\n",
        "\n",
        "      'R2_score': round((r2_e),3),\n",
        "\n",
        "      'Adjusted R2':round((Adjusted_R2_e),2)}\n",
        "training_df=training_df.append(dict1, ignore_index=True)"
      ],
      "metadata": {
        "id": "FiZc0m2K1wJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_e =mean_squared_error(y_test, y_pred_test_en)\n",
        "print(\"MSE:\",MSE_e)\n",
        "\n",
        "#calculate RNSE AHSE_e-np.sqrt(MSE_e) print(\"ANSE:\", RMSE_e)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_e =mean_absolute_error(y_test, y_pred_test_en)\n",
        "print(\"MAE:\",MAE_e)\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#calculate r2 and adjusted r2\n",
        "\n",
        "r2_e= r2_score((y_test), (y_pred_test_en))\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n"
      ],
      "metadata": {
        "id": "Vs9OMZAj1wFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6EF66F0BZ9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict2={'Model': 'Elastic net regression Test',\n",
        "\n",
        "        'MAE': round((MAE_e),3),\n",
        "\n",
        "        'MSE': round((MSE_e),3),\n",
        "\n",
        "        'RMSE': round((RMSE_e),3), 'R2_score': round((r2_e),3),\n",
        "\n",
        "        'Adjusted R2': round((Adjusted_R2_e),2)}\n",
        "\n",
        "test_df=test_df.append(dict2, ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "BF2rpe7s1v99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity- Residual plo\n",
        "\n",
        "plt.scatter((y_pred_test_en), (y_test)-(y_pred_test_en))\n",
        "\n",
        "plt.xlabel('Predicted Values')\n",
        "\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xNM6TFst1v6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.plot(np.array(y_pred_test_en))\n",
        "\n",
        "plt.plot((np.array(y_test)))\n",
        "\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E92eKQlk1v2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECISION TREE\n",
        "\n",
        "A decision tree is a type of supervised machine learning algorithm that is commonly used for classification and regression tasks. It works by recursively splitting the data into subsets based on the values of certain attributes, ultimately arriving at a set of decision rules that can be used to classify or predict outcomes for new data.\n"
      ],
      "metadata": {
        "id": "HJwx4ftq-aKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "decision_regressor= DecisionTreeRegressor(criterion='friedman_mse', max_depth=8,\n",
        "                                          max_features=9, max_leaf_nodes=100,)\n",
        "\n",
        "decision_regressor.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "hPGGMCei1vyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "\n",
        "y_pred_train_d = decision_regressor.predict(X_train)\n",
        "y_pred_test_d =decision_regressor.predict(X_test)\n",
        "\n",
        "print(y_pred_train_d)\n",
        "\n",
        "print(y_pred_test_d)"
      ],
      "metadata": {
        "id": "BHGEPjVu1vty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        " #import the packages from sklearn.metrics import mean_squared_error print(\"Model Score:\", decision_regressor.score(x_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "\n",
        "MSE_d= mean_squared_error(y_train, y_pred_train_d)\n",
        "print(\"MSE:\",MSE_d)\n",
        "\n",
        "\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE:\", RMSE_d)\n",
        "\n",
        "#calculate MAE\n",
        "\n",
        "MAE_d= mean_absolute_error(y_train, y_pred_train_d)\n",
        "print(\"MAE:\", MAE_d)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#calculate r2 and adjusted r2\n",
        "\n",
        "r2_d=r2_score(y_train, y_pred_train_d)\n",
        "\n",
        "\n",
        "print(\"R2:\",r2_d)\n",
        "\n",
        "Adjusted_R2_d=(1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "7-2bbv-i1vqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QM1QwLSbBl_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict1={'Model': 'Dicision tree regression',\n",
        "\n",
        "\"MAE\": round((MAE_d),3),\n",
        "\n",
        "\"MSE\" :round ((MSE_d),3),\n",
        "\n",
        "\"RMSE\": round ((RMSE_d),3 ),\n",
        "\n",
        "'R2_score':round((r2_d),3),\n",
        "\n",
        "'Adjusted R2': round((Adjusted_R2_d),2)\n",
        "\n",
        "}\n",
        "\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "TuyzbmLQ1vmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict2={\"Model\": 'Dicision tree regression',\n",
        "\n",
        "\"MAE\":round ((MAE_d),3),\n",
        "\n",
        "'MSE': round ((MSE_d),3),\n",
        "\n",
        "\"RMSE\":round (( RMSE_d),3),\n",
        "\n",
        "\"R2_score\" : round((r2_d),3),\n",
        "\n",
        "'Adjusted R2': round((Adjusted_R2_d),2)\n",
        "\n",
        "}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "RUkVQ8OgCvwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "\n",
        "plt.scatter((y_pred_test_d), (y_test)-(y_pred_test_d))\n",
        "\n",
        "plt.xlabel('Predicted Values')\n",
        "\n",
        "plt.ylabel('Residuals')\n",
        "\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vC81-i2Q1via"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.plot((np.array(y_pred_test_d)))\n",
        "\n",
        "plt.plot(np.array((y_test)))\n",
        "\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pG24c25y1vde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **conclusion**"
      ],
      "metadata": {
        "id": "ZqwAuZk9W1-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Conclusion\n",
        "\n",
        "During our analysis, we conducted an initial exploratory data analysis (EDA) on all the features in our dataset. Firstly we analysed our\n",
        "\n",
        "dependent variable 'Rented Bike count' and applied transformations as necessar. We then examined the categorical variables and removed those with majority of one class. We also studied the numerical variables, calculated their correlations, distribution and the their relationships with the dependent variable. Additionally we removed some numerical features that contained mostly 0 values and applied one-hot encoding to the categorical variables.\n",
        "\n",
        "Subsequently, we employed 7 machine learning algorithms including Linear Regression, Lasso, Ridge, Elastic Net, Decision Tree."
      ],
      "metadata": {
        "id": "JyCsRW5PXAwY"
      }
    }
  ]
}